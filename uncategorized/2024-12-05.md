# store buffer vs store queue

There is an issue with the scheme I proposed yesterday for handling exceptions. I neglected that synchronous exceptions are subject to the same problems with imprecise state. I was planning to drain the pipeline and simply squash the instructions after the exception. However, once again there can be a store in the execute stage which has already happened at this point.

There are two solutions:

- handle everything in commit (with two states; normal commit and then wait for response from bus)
- decouple exception generating phase (address calculation in execute) from commit (write to store buffer) 

The first is what Cem said yesterday. I don't think this is used in practice because it would grind the performance of the system to a halt. Stores would be executed completely serially, because the instruction is not done until they are read for exceptions. 

The second is the way I believe things are actually done in practice. The *store queue* handles the speculative state. It is a queue of calculated addresses and values. Exceptions are stored here. 

When the store is finally committed (in my case, it's when the store reaches the E2W, in an OOO, it's when the head of the ROB reaches the store), the head of the store queue is dequeued and the store is moved to the store buffer. At this point, the store is architecturally committed. The state used by an asynchronous exception would be at this point.

So I think the solution I will adopt is to do things the right way and introduce a (small) store queue and store buffer. Note that because the cache is modeled as a streaming accelerator (enqueue and dequeue interface), the "store buffer" is really just the cacheReqQ inside the cache. 

I have yet to decide whether I will make the execute stage do a cache access (get the line & etc) or just do a lookup for the address. a load should still be able to do the index lookup concurrently with the tlb access, so we have to keep this in mind. 

The last thing is that because an instruction is officially committed when it's in the store buffer, it can't generate an exception. I'm not sure how this will work with the bus exception that is supposed to be synchronous. if the exception sets the address of the instruction that caused it then it will be a problem. if it just needs to set the address that caused the bus exception then it can be handled asynchronously. (TODO ISA ISSUE)

Note also that exceptions still need to drain the *commit* stage specifically because we do not check the epochs.

# how to design memory subsystem around decoupled access/commit

In light of splitting things from store queue and store buffer, we would need to handle things differently in memory to have the 'optimal' scheme. Right now we just make one request to the memory subsystem on a given address. If the top bits of the address are in the cache region it goes to the cache otherwise it goes to the bus. With the new system, we will have to first make a request with the index, and then another once the tag comes back. Since the top bits of the address are not known until TLB is done, this causes a problem: we make a request to the cache with the index, but we need a way to squash this request if it turns out the PA is in an uncached region.

One possibility is to just resolve the full address in the same cycle. No "request" for the tag. If the address comes back as being in the cached region, put it to the cache, otherwise, the bus. This is definitely a long critical path though as the cache still needs to lookup the tag in this cycle. Note that the index bits are not simply bypassed to the cache because once again, the request should not happen at all unless the top bits are in a cached region. In essence, this is a PIPT cache. I assume "VIPT" is done instead of this in practice due to the critical path constraints.

This is a slight tangent but, I thought that my current cache design was the designed the way it was, where the tags are a Vector of registers while the data is a BRAM, for performance reasons. However, as far as I can tell, it doesn't help. The data on a hit is still not returned until the next cycle. The only thing it helps with is simplifying the logic; if you know whether or not it is a hit, you can choose to do a BRAM read or write for a Store (Miss = Read first, Hit = write only.) But store latency in cache doesn't really matter when you have a store buffer; so they should really both be backed by BRAMs, and you should just always do a read of the line, even if you're having a store hit. (The BRAM also no longer needs to be byte enabled in this case.) I think I just chose this strategy to simplify the logic but I was under the wrong impression that this helps the performance. When you introduce the TLB, things change; a same-cycle tag check helps when the tag arrives one cycle later.

Anyway, this is getting into the details of cache design which I don't particularly care for in this iteration. That whole subsystem will surely be ripped out once jumping to out of order, and it seems like all the techniques to minimize the number of cycles for the cache to respond, and the critical path, are premature optimizations. The point is the main difficulty in implementing a proper, 1-cycle "VIPT" scheme where the TLB access is registered and the virtual index starts first is that you need a way of squashing your "speculation" that the access is supposed to go to cache. 

So instead, I will compromise on the 1-cycle, and treat the cache as a PIPT, waiting for the result from TLB instead of using the VI as soon as I have it, registering the TLB accesses as much as I need. LSU sends to TLB, TLB responds with the full address, and we go to the memory subsystem with the whole address. This simplifies things quite a bit: memory does not need to worry about squashing accesses to the cache, nor does it need to carry info about exceptions. The TLB can be modeled as a very simple streaming accelerator hooked up to the Fetch and LSU stages. 

This will add latency across the board. Once again, I will just go with this solution to avoid premature optimization, but there is one easy way to cope with the poor latency in the Fetch stage: take advantage of the 128-bit word we're still fetching. 